<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.0 Transitional//EN"
  "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">

<html xmlns="http://www.w3.org/1999/xhtml">
<head>
  <title>Using the Cluster</title>
  <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
  <link href="style.css" rel="stylesheet" type="text/css" />
</head>

<body>

<div class="wrapper">

<div align="center"><h1>
Using the Cluster
</h1></div>

<h2>
Cluster Resources
</h2>

<h3>
hardware resources
</h3>

<p>
The following table lists the available hardware resources in the
current Axel system. We currently have <font color="red">14 nodes, 124
CPU cores and over 10TB of disk space</font>. On top of these general
purpose resources, we have 4 GPUs including the NVIDIA Fermi
architecture and 32 FPGA accelerators including Xilinx Virtex-6
architecture. All nodes are supported by UPS (Uninterrupted Power
Supply) and connected through a dedicated Gigabit Ethernet switch.
</p>

<div align="center">
<table border="1">
<tr>
  <th>Node</th>
  <th>CPU</th>
  <th>RAM</th>
  <th>HDD</th>
  <th>GPU</th>
  <th>FPGA</th>
</tr>

<tr>
  <td>cccad1</td>
  <td>Dual Intel Xeon X5650 (6-core per CPU) @ 2.67GHz</td>
  <td>120GB DDR3 @ 1333MHz (ECC)</td>
  <td>1.1TB LVM</td>
  <td>N/A</td>
  <td>N/A</td>
</tr>

<tr>
  <td>cccad2</td>
  <td>Dual Intel Xeon X5650 (6-core per CPU) @ 2.67GHz</td>
  <td>96GB DDR3 @ 1333MHz (ECC)</td>
  <td>1.6TB RAID10</td>
  <td>N/A</td>
  <td>N/A</td>
</tr>

<tr>
  <td>cccad3</td>
  <td>Dual Intel Xeon E5-2650 (8-core per CPU) @ 2.00GHz</td>
  <td>256GB DDR3 @ 1600MHz (ECC)</td>
  <td>2.7TB RAID6</td>
  <td>N/A</td>
  <td>N/A</td>
</tr>

<tr>
  <td>axel01~02</td>
  <td>Dual Intel Xeon E5420 (4-core per CPU) @ 2.50GHz</td>
  <td>16GB DDR2 @ 1066MHz</td>
  <td>250GB</td>
  <td>NVIDIA GeForce GTX 480</td>
  <td>6X Alpha-Data ADM-XRC-5T2</td>
</tr>

<tr>
  <td>axel04</td>
  <td>Intel Core i7 950 (4-core) @ 3.07GHz</td>
  <td>12GB DDR3 @ 1600MHz</td>
  <td>1TB</td>
  <td>NVIDIA Tesla C2070</td>
  <td>N/A</td>
</tr>

<tr>
  <td>axel06</td>
  <td>Intel Core i7 950 (4-Core) @ 3.07GHz</td>
  <td>12GB DDR3 @ 1600MHz</td>
  <td>1TB</td>
  <td>NVIDIA GeForce GTX 580</td>
  <td>N/A</td>
</tr>

<tr>
  <td>axel07</td>
  <td>Intel Core i7 950 (4-core) @ 3.07GHz</td>
  <td>24GB DDR3 @ 1600MHz</td>
  <td>1TB</td>
  <td>NVIDIA GeForce GTX 580</td>
  <td>N/A</td>
</tr>

<!--
<tr>
  <td>axel07 ~ axel09</td>
  <td>Intel Core i7 950 (4-core) @ 3.07GHz</td>
  <td>24GB DDR3 @ 1600MHz</td>
  <td>1TB</td>
  <td>NVIDIA GeForce GTX 580</td>
  <td>N/A</td>
</tr>

<tr>
  <td>axel10</td>
  <td>Intel Core i7 950 (4-core) @ 3.07GHz</td>
  <td>20GB DDR3 @ 1600MHz</td>
  <td>1TB</td>
  <td>NVIDIA GeForce GTX 580</td>
  <td>N/A</td>
</tr>
-->

<tr>
  <td>maxnode1</td>
  <td>Dual Intel Core Xeon X5650 (6-core per CPU) @ 2.67GHz</td>
  <td>48GB DDR3 @ 1600MHz (ECC)</td>
  <td>1TB LVM</td>
  <td>N/A</td>
  <td>4X Maxeler MAX3, MaxelerOS 2012.1</td>
</tr>

<tr>
  <td>maxnode2</td>
  <td>Dual Intel Core Xeon X5650 (6-core per CPU) @ 2.67GHz</td>
  <td>48GB DDR3 @ 1600MHz (ECC)</td>
  <td>1TB LVM</td>
  <td>N/A</td>
  <td>4X Maxeler MAX3, MaxelerOS 2013.2.2</td>
</tr>

<tr>
  <td>maxstation1</td>
  <td>Intel Core i7 870 (4-core) @ 2.93GHz</td>
  <td>16GB DDR3 @ 1600MHz</td>
  <td>500GB LVM</td>
  <td>N/A</td>
  <td>Maxeler MAX3, MaxelerOS 2012.2</td>
</tr>

<tr>
  <td>maxstation2</td>
  <td>Intel Core i7 870 (4-core) @ 2.93GHz</td>
  <td>16GB DDR3 @ 1600MHz</td>
  <td>500GB LVM</td>
  <td>N/A</td>
  <td>Maxeler MAX3, MaxelerOS 2013.2.2</td>
</tr>

<tr>
  <td>maia01</td>
  <td>Dual Intel Xeon E5-2640 (6-core per CPU) @ 2.50GHz</td>
  <td>64GB DDR3 @ 1333MHz (ECC)</td>
  <td>1TB RAID 1</td>
  <td>N/A</td>
  <td>6X Maxeler MAX4, MaxelerOS 2013.2.2</td>
</tr>

<tr>
  <td>pictor</td>
  <td>Intel Core i7 950 (4-core) @ 3.07GHz</td>
  <td>12GB DDR3 @ 1600MHz</td>
  <td>1TB</td>
  <td>NVIDIA GeForce GTX 580</td>
  <td>4X Alpha-Data ADM-XRC-6T1</td>
</tr>

</table>
</div>

<ul>

<li>
cccad1 : for everybody to run multi-threaded jobs.
</li>

<li>
cccad2 and cccad3 : (exclusively) for PhD students or staffs
to run multi-threaded jobs.
</li>

<li>
maxnode1~2 : for everybody to test Maxeler Vectis designs.
</li>

<li>
maia01 : for everybody to test Maxeler Maia designs.
</li>

<li>
axel0* : for everybody to test GPU designs.
</li>

<li>
maxstation1 and maxstation2 : in WPL for everybody to test Maxeler designs.
</li>

<li>
pictor: in WPL for demo purpose and Xilinx Virtex-6 designs.
</li>

</ul>

<h3>
storage resources
</h3>

<p>
In all nodes, your CSG Linux accounts are mounted on <font color="red">
/homes/&ltusername&gt</font> as any other normal CSG managed machines.
The <font color="red">/vol/cc</font> directory is also mount in their
normal locations.
</p>

<p>
For the CC group members, you will be assigned to extra storage through
our group NAS (network attached storage).
You can access the NAS through <font color="red">
/mnt/ccnas/&ltusername&gt</font> and <font color="red">/mnt/data/cccad3/&ltusername&gt</font>.
For /mnt/ccnas/&ltusername&gt, there is no RAID or backup service.
For /mnt/data/cccad3/&ltusername&gt, there is RAID 6 data redundency but no backup service is provided.
Please use the NAS for large temporary files and place your
valuable research data in your personal account.
</p>

<p>
For each node in the Axel cluster, there is a large local storage for
all users in <font color="red">/data</font>. You can consider it as a
temporary directory for your application. There is no simple way to
synchronise the contents in the local storage between different nodes.
</p>

<h3>
software resource
</h3>

<p>
All nodes in Axel are running CentOS (binary compatible to the RedHat
Enterprise Linux distribution) 64-bit (x86_64) Linux. MPICH2 is
selected as the default MPI implementation for distributed applications.
</p>

<p>
The following software are installed in the <font color="red">/vol/cc/opt
</font> directory.
</p>

<pre class="content">
Intel Composer XE 2011 SP1.10.319
Matlab R2012b
MaxCompiler/MaxGenFD/MaxTCP/MaxVM 2012.1
ModelTech ModelSim SE 6.6a
NVIDIA CUDA SDK 5
SystemC 2.3.0
Xilinx ISE System Edition (with Vivado) 13.3/14.2
</pre>

<h2>
Environment Setup
</h2>

<p>
Make sure your <b>.profile</b> or <b>.bash_profile</b> or <b>.bashrc</b>
include the following section.
Change [version] to the one you need.


<pre class="content">
# License Server
export LM_LICENSE_FILE=27000@chicken

# Intel Compiler
source /vol/cc/opt/intel/composer_xe_2011_sp1.10.319/bin/iccvars.sh intel64
export PATH=${PATH}:/vol/cc/opt/intel/composer_xe_2011_sp1.10.319/bin

# Xilinx ISE
export XILINXD_LICENSE_FILE=~/.Xilinx
source /vol/cc/opt/Xilinx/13.3/ISE_DS/settings64.sh

# Maxeler Compiler
source /vol/cc/opt/maxeler/maxcompiler-[version]/settings.sh
export LD_LIBRARY_PATH=${LD_LIBRARY_PATH}:${MAXELEROSDIR}/lib

# NVIDIA CUDA
export PATH=${PATH}:/vol/cc/opt/cuda/bin
</pre>

<p>
A node-locked license for the Xilinx Aurora IP core is required for
building the FPGA design for the Maxeler platform. Users should copy the
file <font color="red">/vol/cc/opt/Xilinx/license/aurora8b10b.lic</font>
to the directory <font color="red">~/.Xilinx/</font>. This allows any
user to build hardware for the Maxeler platform in cccad1~cccad3.
</p>

<p>
The Maxeler licenses cover the following nodes. Other machines, such as
your own desktop, are not supported.
</p>

<pre class="content">
cccad*, axel0*, maxnode*, maxstation*, maia*, pictor
</pre>

<p>
A simple way to check the availabilities of the FlexLM licenses is to
use the flexlm command. For example, to check the status of the Xilinx
ISE license, use can use the following command. The messsage from the
system indicates that we have 25 seats of license and all of them are
available at the moment.
</p>

<pre class="screen">
# <font color="green">lmutil lmstat -a | grep ISE</font>
Users of ISE:  (Total of 25 licenses issued;  Total of 0 licenses in use)
</pre>

<p>
A simple way to check the availabilities of the GPU status is to use the
NVIDIA system management interface command.
</p>

<pre class="screen">
# <font color="green"> nvidia-smi -a</font>
</pre>

<!--
<h2>
Accessing the Compute Nodes
</h2>

<p>
A centralised resource management scheme is supported through the
SLURM CMS (Cluster Management System) tool. All jobs must be submitted
through the head node (currently cccad1). The procedures to run process
remotely on the nodes are detailed below. The overview structure of the
cluster can be checked using the 'sinfo' command.
</p>

<ol>

<li><p>
The login name <i>user_xyz</i> is used in the following examples. Please
replace it with our own login name.  You must put all your program and
data in your home directory <b>/homes/user_xyz</b> which is the same as
on any other CSG managed Linux machines.  Or you can alternatively place
them in our group NAS system <b>/mnt/ccnas//user_xyz</b> which is only
available in the Axel cluster.
</p>

<li><p>
Assume you have the C source code as <i>hello.c</i> and you compile it
to generate the Linux executable as <i>hello </i>.
</p>

<li><p>
Submit the job to the CMS system and monitor the output on screen. The
following command submit the <i>hello</i> executable to a specific
compute node, axel01. The task will be scheduled in the default queue,
which is "axel" in our environment, of the CMS. If you skip the "-w
axel01" option, the CMS will automatically assign your job to a free
node in the default queue. You <b>MUST</b> login the head node (cccad1)
and submit the job from there. After the job is completed, its outputs
are stored in the same directory where you submit it (involved the
"srun" command). Both <i>stdout</i> and <i>stderr</i> is displayed on
the same screen session where you start the "srun" command.
</p>

<pre class="screen">
# <font color="green">srun -w axel01 ./hello</font>
</pre>

<li><p>
User can also submit jobs to the CSM system in batch mode which the
output is directed to a specific file. In this case, the current session
can be terminated (e.g. network interrupt, PC shutdown, etc.) without
affecting the execution of the submitted job. User must first create a
shell script to embed the executable binary in it. The use the following
command to submit the script as normal jobs.
</p>

<pre class="screen">
# <font color="green">sbatch -w axel23 ./hello.sh</font>
</pre>

<li><p>
You can monitor the status of your job using the following command. In
this example, job 77 is running (Status=R) and job 78 is waiting
(Status=PD).
</p>

<pre class="screen">
# <font color="green">squeue</font>
  JOBID PARTITION     NAME     USER  ST       TIME  NODES NODELIST(REASON)
     77      axel    hello user_xyz   R       0:04      1 axel01
     78        i7    hello user_xyz  PD       0:00      1 (Resources)
</pre>

<li><p>
You can cancel a pending job, e.g. the job 78 in the above example, by
the following command.
</p>

<pre class="screen">
# <font color="green">scancel 78</font>
</pre>

<li><p>
Some useful options for submitting jobs are listed below:
</p>

<pre class="content">
-n<xxx>               : spawn xxx instances of the job
-N<xxx>               : spawn xxx instances of the job on xxx different nodes
-w node1,node2,node3  : specify the nodes for the job
--exclusive           : make the node exclusive for the job
-p partition          : specify the partition for the job
</pre>

</ol>

<p>
For advance usage of the CMS software, please refer to the original
<a href="https://computing.llnl.gov/linux/slurm/slurm.html">documents</a>.
</p>
-->

</div>

</body>
</html>
